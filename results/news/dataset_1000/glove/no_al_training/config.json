{
        "word_embedding": "glove",
        "document_embedding": "Pool",
        "neural network": "rnn",
        "dataset": "../data/news/tsv/dataset_1000/",
        "learning_rate": 0.1,
        "max epoch": 100,
        "mini batch size": 16,
        "seed": 5,
        "performance_measure": "g-mean"
}{
                       "alt.atheism": {
                                              "precision": 0.3,
                                              "recall": 0.75,
                                              "f1-score": 0.4285714285714285,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.65,
                                              "recall": 0.65,
                                              "f1-score": 0.65,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7272727272727273,
                                              "recall": 0.8115942028985508,
                                              "f1-score": 0.767123287671233,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.7,
                                              "recall": 0.7368421052631579,
                                              "f1-score": 0.717948717948718,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5098039215686274,
                                              "recall": 0.896551724137931,
                                              "f1-score": 0.65,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.6,
                                              "recall": 0.75,
                                              "f1-score": 0.6666666666666665,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.595,
                       "macro avg": {
                                              "precision": 0.27018716577540103,
                                              "recall": 0.27974940161498196,
                                              "f1-score": 0.2550869336143308,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5126639928698752,
                                              "recall": 0.595,
                                              "f1-score": 0.5384459957850369,
                                              "support": 200
                       }
}{
 "runtime": 61.40988523326814
}{
                       "alt.atheism": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.6111111111111112,
                                              "recall": 0.55,
                                              "f1-score": 0.5789473684210527,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.2,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.25,
                                              "support": 3
                       },
                       "sci.space": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6956521739130435,
                                              "recall": 0.6956521739130435,
                                              "f1-score": 0.6956521739130435,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.5,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.6153846153846154,
                                              "recall": 0.8,
                                              "f1-score": 0.6956521739130435,
                                              "support": 20
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5454545454545454,
                                              "recall": 0.6,
                                              "f1-score": 0.5714285714285713,
                                              "support": 30
                       },
                       "talk.politics.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.6,
                                              "recall": 0.75,
                                              "f1-score": 0.6666666666666665,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.4,
                                              "recall": 0.6666666666666666,
                                              "f1-score": 0.5,
                                              "support": 3
                       },
                       "rec.autos": {
                                              "precision": 0.6,
                                              "recall": 0.75,
                                              "f1-score": 0.6666666666666665,
                                              "support": 4
                       },
                       "accuracy": 0.53,
                       "macro avg": {
                                              "precision": 0.2675467889598324,
                                              "recall": 0.30728260869565216,
                                              "f1-score": 0.28041734771711885,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.49913442113442125,
                                              "recall": 0.53,
                                              "f1-score": 0.5107575732810287,
                                              "support": 200
                       }
}{
 "runtime": 67.82973061688244
}{
                       "alt.atheism": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.6071428571428571,
                                              "recall": 0.85,
                                              "f1-score": 0.7083333333333333,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7671232876712328,
                                              "recall": 0.8115942028985508,
                                              "f1-score": 0.7887323943661971,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.16666666666666666,
                                              "recall": 0.25,
                                              "f1-score": 0.2,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.75,
                                              "recall": 0.45,
                                              "f1-score": 0.5625000000000001,
                                              "support": 20
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5,
                                              "recall": 0.6551724137931034,
                                              "f1-score": 0.5671641791044776,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.1,
                                              "recall": 0.25,
                                              "f1-score": 0.14285714285714288,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.75,
                                              "recall": 0.75,
                                              "f1-score": 0.75,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.56,
                       "macro avg": {
                                              "precision": 0.3487133072407045,
                                              "recall": 0.27583833083458276,
                                              "f1-score": 0.2831222096259147,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.559871819960861,
                                              "recall": 0.56,
                                              "f1-score": 0.5421491010741063,
                                              "support": 200
                       }
}{
 "runtime": 66.13662891834974
}{
                       "alt.atheism": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.7391304347826086,
                                              "recall": 0.85,
                                              "f1-score": 0.7906976744186046,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6708860759493671,
                                              "recall": 0.7794117647058824,
                                              "f1-score": 0.7210884353741497,
                                              "support": 68
                       },
                       "talk.religion.misc": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.4,
                                              "recall": 0.5,
                                              "f1-score": 0.4444444444444445,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.48,
                                              "recall": 0.631578947368421,
                                              "f1-score": 0.5454545454545454,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5769230769230769,
                                              "recall": 0.5172413793103449,
                                              "f1-score": 0.5454545454545454,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.5,
                                              "recall": 0.5,
                                              "f1-score": 0.5,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.2857142857142857,
                                              "recall": 0.5,
                                              "f1-score": 0.36363636363636365,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.555,
                       "macro avg": {
                                              "precision": 0.34513269366846694,
                                              "recall": 0.3139116045692324,
                                              "f1-score": 0.31375308615341835,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5199824411691776,
                                              "recall": 0.555,
                                              "f1-score": 0.5285962568254926,
                                              "support": 200
                       }
}{
 "runtime": 67.06922420673072
}{
                       "alt.atheism": {
                                              "precision": 0.16666666666666666,
                                              "recall": 0.25,
                                              "f1-score": 0.2,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.75,
                                              "recall": 0.75,
                                              "f1-score": 0.75,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 0.5,
                                              "f1-score": 0.6666666666666666,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6547619047619048,
                                              "recall": 0.8088235294117647,
                                              "f1-score": 0.7236842105263157,
                                              "support": 68
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.4838709677419355,
                                              "recall": 0.7894736842105263,
                                              "f1-score": 0.6,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5882352941176471,
                                              "recall": 0.6896551724137931,
                                              "f1-score": 0.6349206349206349,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "accuracy": 0.585,
                       "macro avg": {
                                              "precision": 0.3821767416644077,
                                              "recall": 0.3018976193018042,
                                              "f1-score": 0.3178111946532999,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5322142405349236,
                                              "recall": 0.585,
                                              "f1-score": 0.5430685045948203,
                                              "support": 200
                       }
}{
 "runtime": 67.63827250339091
}{
     "1": 0.7375341730634855,
     "2": 0.6975486336353773,
     "3": 0.7206996908921715,
     "4": 0.7107385650095218,
     "5": 0.7257311106941591
}{
 "overall_perf": 0.7184504346589431
}{
 "overall_runtime": 66.0167482957244
}