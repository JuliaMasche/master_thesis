{
         "word_embedding": "bert",
         "document_embedding": "RNN",
         "neural network": "rnn",
         "dataset": "../data/news/tsv/dataset_1000/",
         "learning_rate": 0.1,
         "max epoch": 100,
         "mini batch size": 16,
         "seed": 5,
         "performance_measure": "g-mean"
}{
                       "alt.atheism": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.64,
                                              "recall": 0.8,
                                              "f1-score": 0.7111111111111111,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 0.5,
                                              "f1-score": 0.6666666666666666,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6703296703296703,
                                              "recall": 0.8840579710144928,
                                              "f1-score": 0.7625000000000001,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.4,
                                              "recall": 0.5,
                                              "f1-score": 0.4444444444444445,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.6470588235294118,
                                              "recall": 0.5789473684210527,
                                              "f1-score": 0.6111111111111113,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.6923076923076923,
                                              "recall": 0.6206896551724138,
                                              "f1-score": 0.6545454545454545,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.42857142857142855,
                                              "recall": 0.75,
                                              "f1-score": 0.5454545454545454,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "accuracy": 0.595,
                       "macro avg": {
                                              "precision": 0.3947467140702434,
                                              "recall": 0.30668474973039794,
                                              "f1-score": 0.31752976190476184,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5620237017884075,
                                              "recall": 0.595,
                                              "f1-score": 0.5593648088023088,
                                              "support": 200
                       }
}{
 "runtime": 156.0245647393167
}{
                       "alt.atheism": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.6,
                                              "f1-score": 0.631578947368421,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.25,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.28571428571428575,
                                              "support": 3
                       },
                       "sci.space": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6770833333333334,
                                              "recall": 0.9420289855072463,
                                              "f1-score": 0.7878787878787878,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.64,
                                              "recall": 0.8,
                                              "f1-score": 0.7111111111111111,
                                              "support": 20
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5111111111111111,
                                              "recall": 0.7666666666666667,
                                              "f1-score": 0.6133333333333334,
                                              "support": 30
                       },
                       "talk.politics.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.6666666666666666,
                                              "f1-score": 0.6666666666666666,
                                              "support": 3
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.595,
                       "macro avg": {
                                              "precision": 0.17057638888888887,
                                              "recall": 0.20543478260869566,
                                              "f1-score": 0.1848141566036303,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.45467708333333334,
                                              "recall": 0.595,
                                              "f1-score": 0.5123729019518494,
                                              "support": 200
                       }
}{
 "runtime": 160.93645183183253
}{
                       "alt.atheism": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.8421052631578947,
                                              "recall": 0.8,
                                              "f1-score": 0.8205128205128205,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.3333333333333333,
                                              "support": 3
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7,
                                              "recall": 0.9130434782608695,
                                              "f1-score": 0.7924528301886793,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.5,
                                              "recall": 0.5,
                                              "f1-score": 0.5,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.75,
                                              "recall": 0.6,
                                              "f1-score": 0.6666666666666665,
                                              "support": 20
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5,
                                              "recall": 0.6551724137931034,
                                              "f1-score": 0.5671641791044776,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.14285714285714285,
                                              "recall": 0.25,
                                              "f1-score": 0.18181818181818182,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.5,
                                              "recall": 0.5,
                                              "f1-score": 0.5,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.16666666666666666,
                                              "recall": 0.25,
                                              "f1-score": 0.2,
                                              "support": 4
                       },
                       "accuracy": 0.61,
                       "macro avg": {
                                              "precision": 0.37591478696741853,
                                              "recall": 0.30257746126936536,
                                              "f1-score": 0.3115497815335889,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5660676691729324,
                                              "recall": 0.61,
                                              "f1-score": 0.5703702971205084,
                                              "support": 200
                       }
}{
 "runtime": 138.25109660439193
}{
                       "alt.atheism": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.782608695652174,
                                              "recall": 0.9,
                                              "f1-score": 0.8372093023255814,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7191011235955056,
                                              "recall": 0.9411764705882353,
                                              "f1-score": 0.8152866242038216,
                                              "support": 68
                       },
                       "talk.religion.misc": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 1.0,
                                              "recall": 0.47368421052631576,
                                              "f1-score": 0.6428571428571429,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5319148936170213,
                                              "recall": 0.8620689655172413,
                                              "f1-score": 0.6578947368421053,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.625,
                       "macro avg": {
                                              "precision": 0.351681235643235,
                                              "recall": 0.27134648233158964,
                                              "f1-score": 0.2855195331685754,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5748829111621574,
                                              "recall": 0.625,
                                              "f1-score": 0.5725274050182485,
                                              "support": 200
                       }
}{
 "runtime": 137.13430971838534
}{
                       "alt.atheism": {
                                              "precision": 0.25,
                                              "recall": 0.5,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.9333333333333333,
                                              "recall": 0.7,
                                              "f1-score": 0.8,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6867469879518072,
                                              "recall": 0.8382352941176471,
                                              "f1-score": 0.7549668874172185,
                                              "support": 68
                       },
                       "talk.religion.misc": {
                                              "precision": 0.4,
                                              "recall": 0.5,
                                              "f1-score": 0.4444444444444445,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.46153846153846156,
                                              "recall": 0.631578947368421,
                                              "f1-score": 0.5333333333333333,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5476190476190477,
                                              "recall": 0.7931034482758621,
                                              "f1-score": 0.647887323943662,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "accuracy": 0.585,
                       "macro avg": {
                                              "precision": 0.3556285581887991,
                                              "recall": 0.2856458844880965,
                                              "f1-score": 0.28831731374264724,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5397448916545302,
                                              "recall": 0.585,
                                              "f1-score": 0.5419022449635265,
                                              "support": 200
                       }
}{
 "runtime": 135.22850372456014
}{
     "1": 0.7330899354897971,
     "2": 0.7274527428662079,
     "3": 0.7435726481697863,
     "4": 0.7549115731712137,
     "5": 0.7282864472887871
}{
 "overall_perf": 0.7374626693971583
}{
 "overall_runtime": 145.51498532369732
}