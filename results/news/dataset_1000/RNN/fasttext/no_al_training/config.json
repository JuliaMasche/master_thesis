{
         "word_embedding": "fasttext",
         "document_embedding": "RNN",
         "neural network": "rnn",
         "dataset": "../data/news/tsv/dataset_1000/",
         "learning_rate": 0.1,
         "max epoch": 100,
         "mini batch size": 16,
         "seed": 5,
         "performance_measure": "g-mean"
}{
                       "alt.atheism": {
                                              "precision": 0.2857142857142857,
                                              "recall": 0.5,
                                              "f1-score": 0.36363636363636365,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.4583333333333333,
                                              "recall": 0.55,
                                              "f1-score": 0.5,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7160493827160493,
                                              "recall": 0.8405797101449275,
                                              "f1-score": 0.7733333333333333,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.25,
                                              "recall": 0.5,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.4074074074074074,
                                              "recall": 0.5789473684210527,
                                              "f1-score": 0.47826086956521735,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.2,
                                              "recall": 0.25,
                                              "f1-score": 0.22222222222222224,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.4827586206896552,
                                              "f1-score": 0.56,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "accuracy": 0.515,
                       "macro avg": {
                                              "precision": 0.2658752204585538,
                                              "recall": 0.23511428496278178,
                                              "f1-score": 0.22915835372357113,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.48962169312169307,
                                              "recall": 0.515,
                                              "f1-score": 0.4888662400401531,
                                              "support": 200
                       }
}{
 "runtime": 59.86125278659165
}{
                       "alt.atheism": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.16666666666666666,
                                              "recall": 0.25,
                                              "f1-score": 0.2,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.65,
                                              "recall": 0.65,
                                              "f1-score": 0.65,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.5,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.4,
                                              "support": 3
                       },
                       "sci.space": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7333333333333333,
                                              "recall": 0.7971014492753623,
                                              "f1-score": 0.7638888888888887,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.52,
                                              "recall": 0.65,
                                              "f1-score": 0.5777777777777778,
                                              "support": 20
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.6410256410256411,
                                              "recall": 0.8333333333333334,
                                              "f1-score": 0.7246376811594204,
                                              "support": 30
                       },
                       "talk.politics.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.1,
                                              "recall": 0.25,
                                              "f1-score": 0.14285714285714288,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.25,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.28571428571428575,
                                              "support": 3
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.56,
                       "macro avg": {
                                              "precision": 0.24471794871794872,
                                              "recall": 0.2298550724637681,
                                              "f1-score": 0.22152950310559008,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5094038461538462,
                                              "recall": 0.56,
                                              "f1-score": 0.5258722394755003,
                                              "support": 200
                       }
}{
 "runtime": 65.2722109053284
}{
                       "alt.atheism": {
                                              "precision": 0.2727272727272727,
                                              "recall": 0.75,
                                              "f1-score": 0.39999999999999997,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.8,
                                              "recall": 0.8,
                                              "f1-score": 0.8000000000000002,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.3333333333333333,
                                              "support": 3
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7261904761904762,
                                              "recall": 0.8840579710144928,
                                              "f1-score": 0.7973856209150327,
                                              "support": 69
                       },
                       "talk.religion.misc": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.5,
                                              "f1-score": 0.5714285714285715,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.5,
                                              "recall": 0.25,
                                              "f1-score": 0.3333333333333333,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.6,
                                              "f1-score": 0.631578947368421,
                                              "support": 20
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 3
                       },
                       "rec.motorcycles": {
                                              "precision": 0.6285714285714286,
                                              "recall": 0.7586206896551724,
                                              "f1-score": 0.6875,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.5,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 0.25,
                                              "recall": 0.25,
                                              "f1-score": 0.25,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.61,
                       "macro avg": {
                                              "precision": 0.28387445887445883,
                                              "recall": 0.29380059970014993,
                                              "f1-score": 0.2768946569856013,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5437997835497835,
                                              "recall": 0.61,
                                              "f1-score": 0.5687053387144332,
                                              "support": 200
                       }
}{
 "runtime": 68.71480179205537
}{
                       "alt.atheism": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.7647058823529411,
                                              "recall": 0.65,
                                              "f1-score": 0.7027027027027027,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.2,
                                              "recall": 0.25,
                                              "f1-score": 0.22222222222222224,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.7011494252873564,
                                              "recall": 0.8970588235294118,
                                              "f1-score": 0.7870967741935484,
                                              "support": 68
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.7142857142857143,
                                              "recall": 0.5263157894736842,
                                              "f1-score": 0.6060606060606061,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.5476190476190477,
                                              "recall": 0.7931034482758621,
                                              "f1-score": 0.647887323943662,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.5,
                                              "recall": 0.5,
                                              "f1-score": 0.5,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.2857142857142857,
                                              "recall": 0.5,
                                              "f1-score": 0.36363636363636365,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.16666666666666666,
                                              "recall": 0.25,
                                              "f1-score": 0.2,
                                              "support": 4
                       },
                       "accuracy": 0.57,
                       "macro avg": {
                                              "precision": 0.2440070510963006,
                                              "recall": 0.2308239030639479,
                                              "f1-score": 0.2214802996379553,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.5051709166425191,
                                              "recall": 0.57,
                                              "f1-score": 0.523119764760837,
                                              "support": 200
                       }
}{
 "runtime": 73.08823780156672
}{
                       "alt.atheism": {
                                              "precision": 0.2,
                                              "recall": 0.25,
                                              "f1-score": 0.22222222222222224,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.crypt": {
                                              "precision": 0.3333333333333333,
                                              "recall": 0.25,
                                              "f1-score": 0.28571428571428575,
                                              "support": 4
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 0.65,
                                              "recall": 0.65,
                                              "f1-score": 0.65,
                                              "support": 20
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.space": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "comp.graphics": {
                                              "precision": 0.6404494382022472,
                                              "recall": 0.8382352941176471,
                                              "f1-score": 0.7261146496815287,
                                              "support": 68
                       },
                       "talk.religion.misc": {
                                              "precision": 0.5,
                                              "recall": 0.5,
                                              "f1-score": 0.5,
                                              "support": 4
                       },
                       "rec.sport.hockey": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.45454545454545453,
                                              "recall": 0.5263157894736842,
                                              "f1-score": 0.4878048780487805,
                                              "support": 19
                       },
                       "comp.windows.x": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.6129032258064516,
                                              "recall": 0.6551724137931034,
                                              "f1-score": 0.6333333333333333,
                                              "support": 29
                       },
                       "talk.politics.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "soc.religion.christian": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 1.0,
                                              "recall": 0.25,
                                              "f1-score": 0.4,
                                              "support": 4
                       },
                       "rec.autos": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 4
                       },
                       "accuracy": 0.525,
                       "macro avg": {
                                              "precision": 0.26956157259437435,
                                              "recall": 0.20848617486922177,
                                              "f1-score": 0.21525946845000754,
                                              "support": 200
                       },
                       "weighted avg": {
                                              "precision": 0.4754722615791843,
                                              "recall": 0.525,
                                              "f1-score": 0.4862125077984173,
                                              "support": 200
                       }
}{
 "runtime": 76.78340561501682
}{
     "1": 0.6860809254222996,
     "2": 0.7181758708468269,
     "3": 0.7489842222131593,
     "4": 0.7199036045814035,
     "5": 0.6850712264056259
}{
 "overall_perf": 0.711643169893863
}{
 "overall_runtime": 68.7439817801118
}