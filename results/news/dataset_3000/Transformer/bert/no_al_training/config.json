{
         "word_embedding": "bert",
         "document_embedding": "Transformer_eng",
         "neural network": "rnn",
         "dataset": "../data/news/tsv/dataset_3000/",
         "learning_rate": 0.1,
         "max epoch": 100,
         "mini batch size": 16,
         "seed": 5,
         "performance_measure": "g-mean"
}{
                       "alt.atheism": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 199
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.9915966386554622,
                                              "recall": 1.0,
                                              "f1-score": 0.9957805907172996,
                                              "support": 118
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.9090909090909091,
                                              "recall": 1.0,
                                              "f1-score": 0.9523809523809523,
                                              "support": 30
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 5
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.graphics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "talk.religion.misc": {
                                              "precision": 1.0,
                                              "recall": 0.6666666666666666,
                                              "f1-score": 0.8,
                                              "support": 6
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 1.0,
                                              "recall": 0.9830508474576272,
                                              "f1-score": 0.9914529914529915,
                                              "support": 118
                       },
                       "comp.windows.x": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 0.8333333333333334,
                                              "recall": 1.0,
                                              "f1-score": 0.9090909090909091,
                                              "support": 5
                       },
                       "talk.politics.misc": {
                                              "precision": 0.9661016949152542,
                                              "recall": 0.9661016949152542,
                                              "f1-score": 0.9661016949152542,
                                              "support": 59
                       },
                       "soc.religion.christian": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 1.0,
                                              "recall": 0.8333333333333334,
                                              "f1-score": 0.9090909090909091,
                                              "support": 6
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "accuracy": 0.9883333333333333,
                       "macro avg": {
                                              "precision": 0.985006128799748,
                                              "recall": 0.9724576271186439,
                                              "f1-score": 0.9761949023824158,
                                              "support": 600
                       },
                       "weighted avg": {
                                              "precision": 0.9890796621678974,
                                              "recall": 0.9883333333333333,
                                              "f1-score": 0.988108318779205,
                                              "support": 600
                       }
}{
 "runtime": 274.2174562141299
}{
                       "alt.atheism": {
                                              "precision": 0.4444444444444444,
                                              "recall": 1.0,
                                              "f1-score": 0.6153846153846153,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 0.9949748743718593,
                                              "f1-score": 0.9974811083123425,
                                              "support": 199
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 1.0,
                                              "recall": 0.9915966386554622,
                                              "f1-score": 0.9957805907172996,
                                              "support": 119
                       },
                       "talk.politics.mideast": {
                                              "precision": 1.0,
                                              "recall": 0.8666666666666667,
                                              "f1-score": 0.9285714285714286,
                                              "support": 30
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 1.0,
                                              "recall": 0.8571428571428571,
                                              "f1-score": 0.923076923076923,
                                              "support": 7
                       },
                       "comp.graphics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "talk.religion.misc": {
                                              "precision": 1.0,
                                              "recall": 0.3333333333333333,
                                              "f1-score": 0.5,
                                              "support": 6
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.9915966386554622,
                                              "recall": 1.0,
                                              "f1-score": 0.9957805907172996,
                                              "support": 118
                       },
                       "comp.windows.x": {
                                              "precision": 0.8,
                                              "recall": 1.0,
                                              "f1-score": 0.888888888888889,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "talk.politics.misc": {
                                              "precision": 0.921875,
                                              "recall": 1.0,
                                              "f1-score": 0.959349593495935,
                                              "support": 59
                       },
                       "soc.religion.christian": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 3
                       },
                       "talk.politics.guns": {
                                              "precision": 1.0,
                                              "recall": 0.8333333333333334,
                                              "f1-score": 0.9090909090909091,
                                              "support": 6
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "accuracy": 0.98,
                       "macro avg": {
                                              "precision": 0.9578958041549953,
                                              "recall": 0.9438523851751756,
                                              "f1-score": 0.9356702324127821,
                                              "support": 600
                       },
                       "weighted avg": {
                                              "precision": 0.9856280102318705,
                                              "recall": 0.98,
                                              "f1-score": 0.9798178122678952,
                                              "support": 600
                       }
}{
 "runtime": 282.5135606043041
}{
                       "alt.atheism": {
                                              "precision": 0.4444444444444444,
                                              "recall": 1.0,
                                              "f1-score": 0.6153846153846153,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 199
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 119
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.9666666666666667,
                                              "recall": 0.9666666666666667,
                                              "f1-score": 0.9666666666666667,
                                              "support": 30
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.graphics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "talk.religion.misc": {
                                              "precision": 0.0,
                                              "recall": 0.0,
                                              "f1-score": 0.0,
                                              "support": 6
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 118
                       },
                       "comp.windows.x": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 5
                       },
                       "talk.politics.misc": {
                                              "precision": 0.9814814814814815,
                                              "recall": 0.9137931034482759,
                                              "f1-score": 0.9464285714285714,
                                              "support": 58
                       },
                       "soc.religion.christian": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 0.5454545454545454,
                                              "recall": 1.0,
                                              "f1-score": 0.7058823529411764,
                                              "support": 6
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "accuracy": 0.98,
                       "macro avg": {
                                              "precision": 0.896902356902357,
                                              "recall": 0.9440229885057472,
                                              "f1-score": 0.9117181103210514,
                                              "support": 600
                       },
                       "weighted avg": {
                                              "precision": 0.9782940516273849,
                                              "recall": 0.98,
                                              "f1-score": 0.977649482870071,
                                              "support": 600
                       }
}{
 "runtime": 278.75069330073893
}{
                       "alt.atheism": {
                                              "precision": 1.0,
                                              "recall": 0.75,
                                              "f1-score": 0.8571428571428571,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 199
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 119
                       },
                       "talk.politics.mideast": {
                                              "precision": 1.0,
                                              "recall": 0.896551724137931,
                                              "f1-score": 0.9454545454545454,
                                              "support": 29
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "comp.graphics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "talk.religion.misc": {
                                              "precision": 0.8333333333333334,
                                              "recall": 0.8333333333333334,
                                              "f1-score": 0.8333333333333334,
                                              "support": 6
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 118
                       },
                       "comp.windows.x": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 5
                       },
                       "talk.politics.misc": {
                                              "precision": 0.9365079365079365,
                                              "recall": 1.0,
                                              "f1-score": 0.9672131147540983,
                                              "support": 59
                       },
                       "soc.religion.christian": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "accuracy": 0.9916666666666667,
                       "macro avg": {
                                              "precision": 0.9884920634920636,
                                              "recall": 0.9739942528735632,
                                              "f1-score": 0.9801571925342417,
                                              "support": 600
                       },
                       "weighted avg": {
                                              "precision": 0.992089947089947,
                                              "recall": 0.9916666666666667,
                                              "f1-score": 0.9915205450287418,
                                              "support": 600
                       }
}{
 "runtime": 280.681665007025
}{
                       "alt.atheism": {
                                              "precision": 1.0,
                                              "recall": 0.5,
                                              "f1-score": 0.6666666666666666,
                                              "support": 4
                       },
                       "sci.electronics": {
                                              "precision": 1.0,
                                              "recall": 0.8333333333333334,
                                              "f1-score": 0.9090909090909091,
                                              "support": 6
                       },
                       "sci.crypt": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 198
                       },
                       "comp.sys.mac.hardware": {
                                              "precision": 1.0,
                                              "recall": 0.75,
                                              "f1-score": 0.8571428571428571,
                                              "support": 4
                       },
                       "rec.sport.baseball": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.sys.ibm.pc.hardware": {
                                              "precision": 0.9915254237288136,
                                              "recall": 0.9831932773109243,
                                              "f1-score": 0.9873417721518987,
                                              "support": 119
                       },
                       "talk.politics.mideast": {
                                              "precision": 0.8709677419354839,
                                              "recall": 0.9310344827586207,
                                              "f1-score": 0.9,
                                              "support": 29
                       },
                       "sci.space": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "comp.os.ms-windows.misc": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 7
                       },
                       "comp.graphics": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "talk.religion.misc": {
                                              "precision": 0.6666666666666666,
                                              "recall": 0.6666666666666666,
                                              "f1-score": 0.6666666666666666,
                                              "support": 6
                       },
                       "rec.sport.hockey": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "misc.forsale": {
                                              "precision": 0.967479674796748,
                                              "recall": 1.0,
                                              "f1-score": 0.9834710743801653,
                                              "support": 119
                       },
                       "comp.windows.x": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "rec.motorcycles": {
                                              "precision": 1.0,
                                              "recall": 0.8,
                                              "f1-score": 0.888888888888889,
                                              "support": 5
                       },
                       "talk.politics.misc": {
                                              "precision": 0.9322033898305084,
                                              "recall": 0.9322033898305084,
                                              "f1-score": 0.9322033898305084,
                                              "support": 59
                       },
                       "soc.religion.christian": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 3
                       },
                       "sci.med": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "talk.politics.guns": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 6
                       },
                       "rec.autos": {
                                              "precision": 1.0,
                                              "recall": 1.0,
                                              "f1-score": 1.0,
                                              "support": 4
                       },
                       "accuracy": 0.975,
                       "macro avg": {
                                              "precision": 0.9714421448479109,
                                              "recall": 0.9198215574950026,
                                              "f1-score": 0.9395736112409281,
                                              "support": 600
                       },
                       "weighted avg": {
                                              "precision": 0.9756327854011181,
                                              "recall": 0.975,
                                              "f1-score": 0.9743682612192394,
                                              "support": 600
                       }
}{
 "runtime": 280.58884516730905
}{
     "1": 0.9936281287877687,
     "2": 0.98926423763991,
     "3": 0.9897484270833572,
     "4": 0.9954541586659361,
     "5": 0.9858590373535571
}{
 "overall_perf": 0.9907907979061058
}{
 "overall_runtime": 279.3504440587014
}